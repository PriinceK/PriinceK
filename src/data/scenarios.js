export const DAILY_SCENARIOS = [
  {
    id: 'day-1-startup-migration',
    title: 'Startup Cloud Migration',
    role: 'Senior Cloud Engineer',
    company: 'FinLeap - a fast-growing fintech startup',
    difficulty: 'Intermediate',
    estimatedTasks: 6,
    briefing: `You've just been hired as the Senior Cloud Engineer at FinLeap, a fintech startup that has been running everything on a single on-premises server. The CTO wants to migrate to GCP before their Series B funding round. The application is a monolithic Node.js app with a PostgreSQL database serving 10,000 daily active users. Your goal today is to plan and begin executing the cloud migration strategy.`,
    objectives: [
      'Assess the current architecture and identify migration priorities',
      'Design a GCP-based architecture that supports growth',
      'Ensure security and compliance for financial data',
      'Set up monitoring and alerting',
    ],
    tasks: [
      {
        id: 'task-1',
        title: 'Choose the right compute platform',
        description: 'The monolithic Node.js application needs a compute home on GCP. The team wants minimal changes to start but needs the flexibility to scale. Which compute service would you choose for the initial migration?',
        options: [
          { id: 'a', text: 'Compute Engine - Lift and shift to VMs', points: 6, feedback: 'Reasonable for a quick migration, but you\'re not taking advantage of managed services. This works as a first step but plan to modernize.' },
          { id: 'b', text: 'Google Kubernetes Engine - Containerize and orchestrate', points: 8, feedback: 'Good choice for a growing team. Containerizing during migration adds initial overhead but provides excellent scalability. Make sure the team has Kubernetes expertise.' },
          { id: 'c', text: 'Cloud Run - Containerize and go serverless', points: 10, feedback: 'Excellent choice! Cloud Run gives you container flexibility with serverless scaling. Perfect for a startup that needs to scale without managing infrastructure. Lower operational overhead than GKE.' },
          { id: 'd', text: 'App Engine - Deploy directly to PaaS', points: 7, feedback: 'App Engine is simple to use but can be restrictive for a fintech app that may need custom networking or specific runtime configurations.' },
        ],
      },
      {
        id: 'task-2',
        title: 'Database migration strategy',
        description: 'The PostgreSQL database contains sensitive financial records. You need to migrate it to a managed service on GCP. Which approach do you recommend?',
        options: [
          { id: 'a', text: 'Cloud SQL for PostgreSQL with private IP and automated backups', points: 10, feedback: 'Perfect choice! Cloud SQL provides a managed PostgreSQL instance with built-in HA, automatic backups, and encryption at rest. Private IP keeps traffic off the public internet.' },
          { id: 'b', text: 'Cloud Spanner for global distribution', points: 5, feedback: 'Spanner is powerful but overkill for 10K DAU. It\'s designed for globally distributed applications needing strong consistency at massive scale. The cost would be prohibitive for a startup.' },
          { id: 'c', text: 'Self-managed PostgreSQL on Compute Engine', points: 3, feedback: 'This defeats the purpose of moving to the cloud. You\'d still be managing patches, backups, replication, and failover manually. Use managed services.' },
          { id: 'd', text: 'Firestore for a NoSQL migration', points: 2, feedback: 'Migrating a relational database to NoSQL requires significant application rewrites and data model changes. This is risky for financial data that relies on relational integrity.' },
        ],
      },
      {
        id: 'task-3',
        title: 'Networking and security setup',
        description: 'Financial data requires strict security controls. How would you configure the network architecture?',
        options: [
          { id: 'a', text: 'Default VPC with firewall rules', points: 3, feedback: 'Never use the default VPC for production, especially for financial data. It has overly permissive defaults and makes auditing difficult.' },
          { id: 'b', text: 'Custom VPC with private subnets, Cloud NAT, and Cloud Armor', points: 10, feedback: 'Excellent! Custom VPC gives you full control. Private subnets keep resources off the internet, Cloud NAT provides outbound access, and Cloud Armor protects against DDoS and application-layer attacks.' },
          { id: 'c', text: 'Custom VPC with all public IPs for simplicity', points: 2, feedback: 'Public IPs on financial services is a major security risk. Always use private networking for sensitive workloads.' },
          { id: 'd', text: 'VPC with VPN tunnel back to on-premises', points: 6, feedback: 'VPN is useful during migration for hybrid connectivity, but shouldn\'t be the long-term strategy. You still need proper VPC configuration with private subnets.' },
        ],
      },
      {
        id: 'task-4',
        title: 'Data encryption strategy',
        description: 'The compliance team requires encryption at rest and in transit for all financial data. How do you implement this?',
        options: [
          { id: 'a', text: 'Rely on GCP default encryption (Google-managed keys)', points: 6, feedback: 'GCP encrypts all data at rest by default, which is good. But for financial compliance, you typically need more control over key management and rotation policies.' },
          { id: 'b', text: 'Customer-Managed Encryption Keys (CMEK) via Cloud KMS', points: 10, feedback: 'Best practice for fintech! CMEK gives you control over encryption key lifecycle, rotation, and access policies while GCP handles the encryption operations. This satisfies most compliance requirements.' },
          { id: 'c', text: 'Customer-Supplied Encryption Keys (CSEK)', points: 7, feedback: 'CSEK gives maximum control but adds significant operational complexity. You\'re responsible for key availability — if you lose the key, you lose the data. CMEK is usually the better balance.' },
          { id: 'd', text: 'Application-level encryption only', points: 4, feedback: 'Application-level encryption can complement infrastructure encryption but shouldn\'t replace it. You need defense in depth — encrypt at the infrastructure level too.' },
        ],
      },
      {
        id: 'task-5',
        title: 'CI/CD pipeline setup',
        description: 'The team currently deploys manually via SSH. You need to set up a proper CI/CD pipeline. What\'s your approach?',
        options: [
          { id: 'a', text: 'Cloud Build with Cloud Deploy for automated deployments', points: 10, feedback: 'Excellent choice! Cloud Build handles CI (build + test) while Cloud Deploy handles CD with approval gates and canary deployments. Fully managed and integrates natively with GCP.' },
          { id: 'b', text: 'Jenkins on a Compute Engine VM', points: 4, feedback: 'Jenkins works but is yet another thing to manage. You\'d need to maintain the Jenkins server, plugins, security patches, and scaling. Use managed CI/CD services instead.' },
          { id: 'c', text: 'GitHub Actions with manual kubectl commands', points: 5, feedback: 'GitHub Actions is fine for CI, but manual kubectl deployments aren\'t a proper CD strategy. You need automated, auditable deployments with rollback capability.' },
          { id: 'd', text: 'Cloud Build with Artifact Registry and automated rollouts', points: 9, feedback: 'Great choice! Cloud Build + Artifact Registry gives you a solid CI pipeline with versioned container images. Add deployment automation for full CD.' },
        ],
      },
      {
        id: 'task-6',
        title: 'Monitoring and observability',
        description: 'Production systems need observability. How do you set up monitoring for the migrated application?',
        options: [
          { id: 'a', text: 'Cloud Monitoring + Cloud Logging + Error Reporting with custom dashboards and alerts', points: 10, feedback: 'Complete observability stack! Cloud Monitoring for metrics, Cloud Logging for logs, and Error Reporting for exceptions. Custom dashboards give the team visibility, and alerts ensure rapid incident response.' },
          { id: 'b', text: 'Only Cloud Logging for debugging', points: 3, feedback: 'Logs alone aren\'t sufficient. You need metrics for performance tracking, alerts for incident response, and distributed tracing for debugging complex issues.' },
          { id: 'c', text: 'Third-party monitoring tool (Datadog/New Relic)', points: 7, feedback: 'Third-party tools are powerful but add cost and complexity. For a startup, the native GCP monitoring stack is usually sufficient and better integrated. Consider third-party tools as you scale.' },
          { id: 'd', text: 'Custom Prometheus + Grafana on GKE', points: 5, feedback: 'Self-managed monitoring adds operational overhead. For a startup trying to move fast, managed monitoring services are preferred. Google Cloud Managed Prometheus is a better middle ground if you want the Prometheus ecosystem.' },
        ],
      },
    ],
  },
  {
    id: 'day-2-scaling-ecommerce',
    title: 'E-Commerce Black Friday Prep',
    role: 'Cloud Architect',
    company: 'ShopWave - a mid-size e-commerce platform',
    difficulty: 'Advanced',
    estimatedTasks: 6,
    briefing: `ShopWave's Black Friday sale is in 2 weeks. Last year, the site went down for 3 hours during peak traffic, costing $2M in lost revenue. You're the Cloud Architect responsible for ensuring the platform handles a 20x traffic spike this year. The current setup uses GKE with Cloud SQL, but it buckled under load. The CEO has made it clear: downtime is not an option.`,
    objectives: [
      'Design auto-scaling strategy for 20x traffic spikes',
      'Implement caching and CDN for performance',
      'Set up database read replicas and connection pooling',
      'Create a load testing and chaos engineering plan',
    ],
    tasks: [
      {
        id: 'task-1',
        title: 'GKE auto-scaling configuration',
        description: 'The GKE cluster needs to handle 20x traffic. Currently it runs 3 nodes with no auto-scaling. How do you configure scaling?',
        options: [
          { id: 'a', text: 'Cluster Autoscaler + Horizontal Pod Autoscaler (HPA) + Pod Disruption Budgets', points: 10, feedback: 'Perfect! The trifecta: Cluster Autoscaler adds/removes nodes, HPA scales pods based on metrics, and PDBs ensure availability during scaling. Set appropriate min/max values and pre-warm before Black Friday.' },
          { id: 'b', text: 'Manually scale to 60 nodes before Black Friday', points: 4, feedback: 'Static over-provisioning wastes money and doesn\'t handle unexpected traffic patterns. What if traffic is 25x instead of 20x? Auto-scaling is essential.' },
          { id: 'c', text: 'Vertical Pod Autoscaler only', points: 3, feedback: 'VPA requires pod restarts to resize, causing brief downtime. For a live e-commerce site during peak traffic, this is risky. HPA is better for handling traffic spikes.' },
          { id: 'd', text: 'HPA only without Cluster Autoscaler', points: 6, feedback: 'HPA scales pods but without Cluster Autoscaler, you\'ll run out of node resources. Pods will stay Pending if there aren\'t enough nodes to schedule them on.' },
        ],
      },
      {
        id: 'task-2',
        title: 'Caching strategy',
        description: 'Product pages and catalog data are fetched from the database on every request. How do you implement caching?',
        options: [
          { id: 'a', text: 'Memorystore for Redis as application cache + Cloud CDN for static assets', points: 10, feedback: 'Excellent layered approach! Redis handles dynamic content caching (product data, sessions, cart), while Cloud CDN caches static assets at edge locations globally. This dramatically reduces database load.' },
          { id: 'b', text: 'Cloud CDN only', points: 5, feedback: 'CDN helps with static assets but doesn\'t help with dynamic product data, user sessions, or shopping cart data. You need an application-level cache too.' },
          { id: 'c', text: 'Application-level in-memory caching per pod', points: 4, feedback: 'Per-pod caching doesn\'t share state across pods, leading to inconsistent data and redundant cache population. A centralized cache like Redis is more efficient.' },
          { id: 'd', text: 'Cache everything in Firestore', points: 3, feedback: 'Firestore is a database, not a cache. While it\'s fast, it doesn\'t provide the sub-millisecond latency of an in-memory cache and adds cost per read operation.' },
        ],
      },
      {
        id: 'task-3',
        title: 'Database scaling for peak load',
        description: 'Cloud SQL is the bottleneck. Read queries dominate (catalog browsing) but writes spike during checkout. How do you scale the database?',
        options: [
          { id: 'a', text: 'Cloud SQL read replicas + PgBouncer for connection pooling + vertical scaling of primary', points: 10, feedback: 'Comprehensive approach! Read replicas handle catalog queries, PgBouncer prevents connection exhaustion (critical at 20x scale), and vertical scaling ensures the primary handles write spikes. Consider also using Cloud SQL Insights for query optimization.' },
          { id: 'b', text: 'Migrate to Cloud Spanner', points: 5, feedback: 'Spanner would solve scaling but requires significant application changes and a risky migration 2 weeks before Black Friday. This is a post-event optimization, not an emergency fix.' },
          { id: 'c', text: 'Just increase Cloud SQL machine type', points: 4, feedback: 'Vertical scaling alone has limits. A single machine can\'t handle 20x concurrent connections efficiently, and you\'re creating a single point of failure.' },
          { id: 'd', text: 'Add read replicas only', points: 7, feedback: 'Read replicas help with catalog queries, but without connection pooling, you\'ll hit Cloud SQL\'s connection limit under peak load. Connection pooling is critical at scale.' },
        ],
      },
      {
        id: 'task-4',
        title: 'Global load balancing configuration',
        description: 'ShopWave has customers worldwide. How do you configure load balancing for the sale?',
        options: [
          { id: 'a', text: 'Global HTTP(S) Load Balancer with Cloud CDN, health checks, and Cloud Armor', points: 10, feedback: 'Complete solution! Global LB routes users to the nearest healthy backend, Cloud CDN caches at the edge, health checks ensure traffic only goes to healthy instances, and Cloud Armor protects against DDoS — critical during a high-profile sale.' },
          { id: 'b', text: 'Regional TCP Load Balancer', points: 4, feedback: 'TCP-level balancing misses HTTP-level features like URL-based routing, SSL termination, and CDN integration. For a web application, use HTTP(S) Load Balancing.' },
          { id: 'c', text: 'Cloud DNS round-robin', points: 2, feedback: 'DNS round-robin doesn\'t perform health checks, can\'t handle session affinity, and DNS propagation delays make it unsuitable for rapid scaling or failover.' },
          { id: 'd', text: 'Global HTTP(S) Load Balancer with Cloud Armor only', points: 7, feedback: 'Good start with the global LB and DDoS protection, but you\'re missing CDN integration and should also configure health checks with appropriate thresholds for rapid failover.' },
        ],
      },
      {
        id: 'task-5',
        title: 'Pre-sale load testing',
        description: 'How do you validate the infrastructure can handle 20x traffic before the actual sale?',
        options: [
          { id: 'a', text: 'Use a load testing tool to simulate 20x traffic with realistic user journeys, monitor all metrics, and run chaos engineering experiments', points: 10, feedback: 'Thorough approach! Simulating realistic traffic (browse, add to cart, checkout) reveals bottlenecks that synthetic tests miss. Chaos engineering (killing pods, simulating failures) validates resilience. Monitor database connections, latency percentiles, and error rates.' },
          { id: 'b', text: 'Simple HTTP benchmarking (e.g., ab or wrk) against the homepage', points: 3, feedback: 'Benchmarking a single endpoint doesn\'t test the real user flow. You need to simulate realistic journeys including catalog browsing, cart management, and checkout with payment processing.' },
          { id: 'c', text: 'Scale to 20x and check if pods start', points: 2, feedback: 'Pods starting doesn\'t mean the system works under load. You need to verify the entire stack: database connections, cache hit rates, API latency, and payment processing under concurrent load.' },
          { id: 'd', text: 'Load test at 10x and hope 20x works', points: 4, feedback: 'Scaling is rarely linear. Problems at 20x (connection limits, memory pressure, lock contention) may not appear at 10x. Always test at or above your expected peak.' },
        ],
      },
      {
        id: 'task-6',
        title: 'Incident response plan',
        description: 'Despite preparation, things can still go wrong. What\'s your incident response strategy?',
        options: [
          { id: 'a', text: 'Alerting policies in Cloud Monitoring, documented runbooks, designated on-call rotation, and pre-approved emergency scaling limits', points: 10, feedback: 'Comprehensive incident management! Alerts catch issues early, runbooks reduce response time by giving clear steps, on-call rotation ensures coverage, and pre-approved scaling limits avoid bureaucratic delays during a crisis.' },
          { id: 'b', text: 'Everyone monitors dashboards during the sale', points: 3, feedback: 'Dashboard watching leads to alert fatigue and missed issues. Automated alerts with clear escalation paths are far more reliable than human monitoring alone.' },
          { id: 'c', text: 'Set up PagerDuty alerts and hope for the best', points: 5, feedback: 'Alerting is necessary but not sufficient. Without runbooks, on-call engineers won\'t know what to do. Without pre-approved actions, they may waste time getting approvals during an incident.' },
          { id: 'd', text: 'Automated remediation with Cloud Functions triggered by monitoring alerts', points: 8, feedback: 'Automated remediation is advanced and powerful for known failure modes (e.g., auto-restart crashed pods, scale up on traffic spikes). But you still need human oversight for novel issues. Combine with documented runbooks for the best outcome.' },
        ],
      },
    ],
  },
  {
    id: 'day-3-data-platform',
    title: 'Building a Data Analytics Platform',
    role: 'Data Platform Engineer',
    company: 'HealthMetrics - a healthcare analytics company',
    difficulty: 'Advanced',
    estimatedTasks: 6,
    briefing: `HealthMetrics processes patient health data from 500 hospitals nationwide. Currently, data arrives via nightly batch transfers and is analyzed in spreadsheets. The medical research team needs real-time analytics, ML-based anomaly detection, and strict HIPAA compliance. You're building the data platform from scratch on GCP.`,
    objectives: [
      'Design a HIPAA-compliant data ingestion pipeline',
      'Build real-time and batch processing capabilities',
      'Set up a data warehouse for analytics',
      'Implement ML-based anomaly detection',
    ],
    tasks: [
      {
        id: 'task-1',
        title: 'Data ingestion architecture',
        description: 'Hospital systems send HL7/FHIR health data. Some hospitals send real-time streams, others send nightly batch files. How do you design ingestion?',
        options: [
          { id: 'a', text: 'Pub/Sub for real-time streams + Cloud Storage for batch files, both feeding into Dataflow', points: 10, feedback: 'Excellent! Pub/Sub handles real-time ingestion with guaranteed delivery, Cloud Storage handles batch uploads with event notifications, and Dataflow provides unified stream/batch processing with the same Apache Beam pipeline code.' },
          { id: 'b', text: 'All data through Pub/Sub, even batch files', points: 6, feedback: 'Pub/Sub works for both but isn\'t ideal for large batch files. Cloud Storage is cheaper and better for large file transfers, with event notifications to trigger processing.' },
          { id: 'c', text: 'Direct database writes from hospital systems', points: 2, feedback: 'Direct DB writes create tight coupling, can\'t handle schema variations across hospitals, and don\'t provide the buffering needed when hospitals send large batches.' },
          { id: 'd', text: 'FTP server on Compute Engine for all data', points: 1, feedback: 'FTP is insecure, not scalable, and requires manual processing. For healthcare data, you need encrypted, auditable, managed ingestion services.' },
        ],
      },
      {
        id: 'task-2',
        title: 'Data processing pipeline',
        description: 'Raw health data needs validation, de-identification, transformation, and loading. How do you build the processing pipeline?',
        options: [
          { id: 'a', text: 'Dataflow (Apache Beam) with separate pipelines for validation, de-identification, and loading', points: 10, feedback: 'Perfect! Dataflow provides autoscaling, exactly-once processing, and can handle both stream and batch. Separate pipeline stages make debugging easier and allow independent scaling.' },
          { id: 'b', text: 'Cloud Composer (Airflow) orchestrating Cloud Functions', points: 7, feedback: 'Good for batch orchestration, but Cloud Functions have timeout limits and aren\'t ideal for heavy data processing. Dataflow is better suited for data pipeline workloads.' },
          { id: 'c', text: 'Dataproc (Spark) jobs scheduled via cron', points: 5, feedback: 'Spark works for batch but adding real-time is complex. Dataflow\'s unified model handles both. Also, cron scheduling lacks retry logic, dependency management, and monitoring.' },
          { id: 'd', text: 'Custom Python scripts on Compute Engine', points: 2, feedback: 'Custom scripts don\'t scale, aren\'t fault-tolerant, and create maintenance burden. Managed services like Dataflow handle scaling, retries, and monitoring automatically.' },
        ],
      },
      {
        id: 'task-3',
        title: 'Data warehouse design',
        description: 'Researchers need to run complex analytical queries across millions of patient records. Which warehouse solution and schema design do you choose?',
        options: [
          { id: 'a', text: 'BigQuery with a star schema, partitioned by date and clustered by hospital_id', points: 10, feedback: 'Optimal design! BigQuery\'s serverless architecture handles massive analytical queries. Star schema optimizes for analytical queries. Partitioning by date reduces scan costs, and clustering by hospital_id speeds up filtered queries. Use authorized views for access control.' },
          { id: 'b', text: 'BigQuery with a flat denormalized table', points: 6, feedback: 'Denormalized tables work in BigQuery but waste storage with repeated data and make updates difficult. Star schema provides better query flexibility and maintains data integrity.' },
          { id: 'c', text: 'Cloud SQL as the data warehouse', points: 2, feedback: 'Relational databases aren\'t designed for analytical workloads on millions of records. Query performance degrades significantly. Use a columnar data warehouse like BigQuery.' },
          { id: 'd', text: 'Bigtable for analytical queries', points: 3, feedback: 'Bigtable excels at high-throughput reads/writes with known key patterns but doesn\'t support SQL or complex analytical queries. BigQuery is the right tool for analytics.' },
        ],
      },
      {
        id: 'task-4',
        title: 'HIPAA compliance architecture',
        description: 'Healthcare data requires HIPAA compliance. How do you ensure the GCP architecture meets regulatory requirements?',
        options: [
          { id: 'a', text: 'BAA with Google, CMEK encryption, VPC Service Controls, audit logging, DLP API for PHI detection, and access transparency', points: 10, feedback: 'Comprehensive compliance! BAA is legally required. CMEK controls encryption keys. VPC Service Controls prevent data exfiltration. Audit logging creates accountability trails. DLP API helps detect and protect PHI. Access Transparency shows when Google staff access your data.' },
          { id: 'b', text: 'Just sign the BAA and use default GCP security', points: 4, feedback: 'The BAA is necessary but not sufficient. HIPAA requires technical safeguards including access controls, audit trails, encryption management, and data loss prevention.' },
          { id: 'c', text: 'Encrypt everything and restrict IAM access', points: 6, feedback: 'Good start but missing key HIPAA requirements: audit logging, data loss prevention, VPC service controls to prevent exfiltration, and the BAA itself.' },
          { id: 'd', text: 'Keep all data on-premises and only process on GCP', points: 3, feedback: 'This hybrid approach adds complexity without reducing compliance requirements. If data touches GCP for processing, you need the same compliance controls. Better to go fully managed with proper compliance.' },
        ],
      },
      {
        id: 'task-5',
        title: 'ML anomaly detection',
        description: 'The research team wants to detect anomalous patient vitals in real-time. How do you implement ML-based anomaly detection?',
        options: [
          { id: 'a', text: 'Vertex AI for model training, deploy to an endpoint, and call from Dataflow pipeline for real-time scoring', points: 10, feedback: 'Excellent architecture! Vertex AI provides managed training and deployment. Calling the model from the Dataflow pipeline means anomalies are detected as data streams in, enabling real-time alerting. Use Vertex AI Pipelines for model retraining.' },
          { id: 'b', text: 'BigQuery ML for training and batch scoring', points: 7, feedback: 'BQML is great for batch analytics but doesn\'t support real-time scoring. For real-time anomaly detection on streaming data, you need an online prediction endpoint.' },
          { id: 'c', text: 'Custom TensorFlow model on a Compute Engine GPU', points: 4, feedback: 'Self-managed ML infrastructure is complex and expensive. You\'d need to handle model serving, scaling, monitoring, and A/B testing. Vertex AI manages all of this.' },
          { id: 'd', text: 'Pre-built anomaly detection API', points: 6, feedback: 'Pre-built APIs are quick to start but may not be suitable for specialized medical data. Custom models trained on healthcare-specific patterns typically perform better for domain-specific anomaly detection.' },
        ],
      },
      {
        id: 'task-6',
        title: 'Data governance and access control',
        description: 'Different teams need different levels of access: researchers need anonymized data, clinicians need full records, and admins need audit access. How do you manage this?',
        options: [
          { id: 'a', text: 'BigQuery authorized views per role, column-level security, row-level security, and Data Catalog for discovery', points: 10, feedback: 'Best practice! Authorized views expose only appropriate columns per role. Column-level security protects sensitive fields. Row-level security limits records by hospital/region. Data Catalog helps teams discover available datasets with proper access controls.' },
          { id: 'b', text: 'Separate BigQuery datasets per team with copied data', points: 5, feedback: 'Data duplication creates consistency issues and increases storage costs. Use views and security policies on a single source of truth instead.' },
          { id: 'c', text: 'IAM roles at the dataset level only', points: 4, feedback: 'Dataset-level IAM is too coarse. Researchers would see PHI they shouldn\'t, or be blocked from data they need. You need column and row-level controls.' },
          { id: 'd', text: 'Application-level access control with a custom middleware', points: 3, feedback: 'Custom middleware is fragile, hard to audit, and doesn\'t protect against direct BigQuery access. Use BigQuery\'s native security features for defense in depth.' },
        ],
      },
    ],
  },
  {
    id: 'day-4-microservices',
    title: 'Microservices Decomposition',
    role: 'Platform Engineer',
    company: 'TravelNow - an online travel booking platform',
    difficulty: 'Intermediate',
    estimatedTasks: 6,
    briefing: `TravelNow's monolithic Java application handles flight search, hotel booking, payment processing, and user management. Deployments take 4 hours and a bug in one module brings down the entire platform. The VP of Engineering has decided to decompose into microservices on GCP. You're leading the platform team responsible for the architecture, infrastructure, and developer experience.`,
    objectives: [
      'Design the microservices architecture and communication patterns',
      'Set up the container platform and service mesh',
      'Implement a CI/CD pipeline for independent deployments',
      'Design for resilience with circuit breakers and graceful degradation',
    ],
    tasks: [
      {
        id: 'task-1',
        title: 'Container orchestration platform',
        description: 'The microservices need a container platform. The team has 8 developers with varying Kubernetes experience. Which platform do you choose?',
        options: [
          { id: 'a', text: 'GKE Autopilot for fully managed node management with Anthos Service Mesh', points: 10, feedback: 'Excellent choice! GKE Autopilot eliminates node management entirely — Google handles node provisioning, scaling, and security. Anthos Service Mesh provides observability, traffic management, and mTLS between services. Great for a team with mixed K8s experience.' },
          { id: 'b', text: 'GKE Standard with manual node pools', points: 6, feedback: 'GKE Standard works but requires the team to manage node pools, scaling policies, and node security. With mixed K8s experience, this adds unnecessary operational burden.' },
          { id: 'c', text: 'Cloud Run for each microservice', points: 8, feedback: 'Cloud Run is great for individual services and eliminates infrastructure management. However, you lose fine-grained inter-service networking, service mesh capabilities, and may face cold start issues for latency-sensitive booking flows.' },
          { id: 'd', text: 'Self-managed Kubernetes on Compute Engine', points: 2, feedback: 'Self-managed K8s is a full-time job. Your team should focus on building business value, not managing etcd, control planes, and node operating systems.' },
        ],
      },
      {
        id: 'task-2',
        title: 'Inter-service communication',
        description: 'Microservices need to communicate. Flight search calls the pricing service, booking calls payment, etc. How do you design communication?',
        options: [
          { id: 'a', text: 'Synchronous gRPC for real-time requests + Pub/Sub for async events (booking confirmations, notifications)', points: 10, feedback: 'Perfect hybrid approach! gRPC provides fast, type-safe synchronous calls for user-facing flows (search, price check). Pub/Sub decouples async operations (email confirmations, analytics events). This prevents cascading failures from slow consumers.' },
          { id: 'b', text: 'REST APIs for everything', points: 5, feedback: 'REST works but is verbose and slower than gRPC for internal service-to-service calls. It also lacks built-in streaming and strong typing. For user-facing APIs, REST is fine, but internal communication benefits from gRPC.' },
          { id: 'c', text: 'All communication through Pub/Sub', points: 4, feedback: 'Fully async architecture adds complexity for request/response patterns. Users can\'t wait for async responses during search or checkout. Use sync for interactive flows and async for background processing.' },
          { id: 'd', text: 'Shared database instead of direct communication', points: 1, feedback: 'A shared database creates tight coupling between services — the exact problem you\'re trying to solve. Each service should own its data and expose it through APIs.' },
        ],
      },
      {
        id: 'task-3',
        title: 'CI/CD for microservices',
        description: 'Each microservice needs independent deployments. The monolith took 4 hours to deploy. How do you set up CI/CD?',
        options: [
          { id: 'a', text: 'Cloud Build per service with Artifact Registry, Cloud Deploy for progressive rollouts (canary + approval gates)', points: 10, feedback: 'Excellent! Per-service Cloud Build configs enable independent builds. Artifact Registry stores versioned images. Cloud Deploy automates progressive rollouts with canary deployments that catch issues before full rollout. Approval gates add safety for critical services like payments.' },
          { id: 'b', text: 'Single Cloud Build pipeline for all services', points: 3, feedback: 'A single pipeline means all services deploy together — this is the monolith deployment problem you\'re trying to escape. Each service needs its own pipeline for independent releases.' },
          { id: 'c', text: 'GitHub Actions with kubectl apply', points: 5, feedback: 'GitHub Actions can handle CI, but raw kubectl apply doesn\'t provide progressive rollouts, automatic rollback, or deployment history. You need proper CD tooling for production microservices.' },
          { id: 'd', text: 'Cloud Build with Artifact Registry and rolling updates', points: 8, feedback: 'Good setup for CI and basic deployments. Rolling updates are an improvement over big-bang deploys. Consider adding canary deployments for higher confidence in releases.' },
        ],
      },
      {
        id: 'task-4',
        title: 'Database per service strategy',
        description: 'Each microservice needs its own data store. How do you manage data for the different services?',
        options: [
          { id: 'a', text: 'Cloud SQL for booking/payment (relational), Firestore for user profiles (flexible schema), Memorystore for search caching', points: 10, feedback: 'Polyglot persistence done right! Relational data (bookings, payments) fits Cloud SQL. User profiles with varying attributes fit Firestore. Search results benefit from Redis caching for sub-millisecond response times. Each service chooses the best store for its data patterns.' },
          { id: 'b', text: 'Cloud SQL for everything', points: 5, feedback: 'Cloud SQL works for all services but you\'re forcing relational patterns on data that may not need it. Search caching in SQL is slow, and user profiles with flexible schemas are awkward in rigid relational tables.' },
          { id: 'c', text: 'Firestore for everything', points: 4, feedback: 'Firestore isn\'t ideal for transactional payment processing or complex relational booking queries. Use the right database for each service\'s data access patterns.' },
          { id: 'd', text: 'One shared Cloud SQL instance with separate schemas', points: 3, feedback: 'Shared infrastructure creates coupling. One service\'s heavy queries can impact another\'s performance. Schema migrations become coordination nightmares. Separate databases enable true independence.' },
        ],
      },
      {
        id: 'task-5',
        title: 'Resilience patterns',
        description: 'If the payment service is slow, it shouldn\'t bring down flight search. How do you implement resilience?',
        options: [
          { id: 'a', text: 'Circuit breakers, retry with exponential backoff, bulkheads, timeouts, and graceful degradation per service', points: 10, feedback: 'Complete resilience toolkit! Circuit breakers prevent cascade failures. Exponential backoff avoids thundering herds. Bulkheads isolate failures. Timeouts prevent hanging. Graceful degradation means search works even if recommendations are down. Anthos Service Mesh can handle many of these.' },
          { id: 'b', text: 'Retry logic in each service', points: 4, feedback: 'Retries alone can cause thundering herd problems and amplify failures. Without circuit breakers, a failing service gets hammered with retries from all callers, making the situation worse.' },
          { id: 'c', text: 'Set generous timeouts to wait for slow services', points: 2, feedback: 'Long timeouts tie up connections and threads, causing the calling service to degrade too. This is how cascading failures happen. Use short timeouts with circuit breakers instead.' },
          { id: 'd', text: 'Circuit breakers and timeouts only', points: 7, feedback: 'Good foundation, but you\'re missing graceful degradation (showing cached results when a service is down) and bulkhead isolation. Complete resilience requires multiple patterns working together.' },
        ],
      },
      {
        id: 'task-6',
        title: 'Observability for distributed systems',
        description: 'With multiple services, debugging is harder. A user reports slow bookings. How do you set up observability?',
        options: [
          { id: 'a', text: 'Distributed tracing (Cloud Trace), structured logging (Cloud Logging with correlation IDs), metrics per service (Cloud Monitoring), and SLO-based alerting', points: 10, feedback: 'Complete observability! Distributed tracing shows the full request path across services. Correlation IDs connect logs across services. Per-service metrics reveal which service is the bottleneck. SLO-based alerting focuses on user-impacting issues rather than noise.' },
          { id: 'b', text: 'Cloud Logging with grep-based debugging', points: 3, feedback: 'Grep-based debugging across multiple services is a nightmare. Without trace IDs, you can\'t follow a request across services. Without metrics, you can\'t identify which service is the bottleneck.' },
          { id: 'c', text: 'Cloud Monitoring dashboards for each service', points: 5, feedback: 'Dashboards show service-level metrics but don\'t help trace individual requests across services. You need distributed tracing to debug end-to-end latency issues.' },
          { id: 'd', text: 'Anthos Service Mesh observability features only', points: 6, feedback: 'ASM provides good service-level metrics and tracing but doesn\'t capture application-level details. Combine with structured logging and custom metrics for complete observability.' },
        ],
      },
    ],
  },
  {
    id: 'day-5-disaster-recovery',
    title: 'Disaster Recovery Architecture',
    role: 'Reliability Engineer',
    company: 'GlobalBank - a multinational banking institution',
    difficulty: 'Expert',
    estimatedTasks: 6,
    briefing: `GlobalBank operates critical banking infrastructure serving 50 million customers across 30 countries. Regulators require a Recovery Time Objective (RTO) of 15 minutes and Recovery Point Objective (RPO) of zero for core banking services. You're designing the disaster recovery architecture on GCP that will satisfy auditors and keep the bank running if an entire region goes down.`,
    objectives: [
      'Design a multi-region active-active architecture',
      'Implement zero-RPO data replication',
      'Create automated failover procedures',
      'Build DR testing and validation processes',
    ],
    tasks: [
      {
        id: 'task-1',
        title: 'Multi-region compute strategy',
        description: 'Core banking services must survive a full region failure. How do you architect the compute layer?',
        options: [
          { id: 'a', text: 'Active-active GKE clusters in 3 regions with Global Load Balancer and automated failover', points: 10, feedback: 'Best approach for zero RPO! Active-active means all regions serve traffic simultaneously. If one fails, the Global LB routes to surviving regions with no manual intervention. Three regions provides N+2 redundancy and satisfies most regulatory requirements.' },
          { id: 'b', text: 'Active-passive with a primary and standby region', points: 6, feedback: 'Active-passive means the standby region may have cold-start issues (scaling, cache warming) that exceed the 15-minute RTO. Active-active eliminates this risk since all regions are already serving traffic.' },
          { id: 'c', text: 'Single region with multiple zones', points: 3, feedback: 'Multi-zone protects against zone failures but not region-level disasters (network partition, power grid failure). For banking regulatory requirements, multi-region is mandatory.' },
          { id: 'd', text: 'Active-active in 2 regions', points: 8, feedback: 'Two regions works but provides no buffer if one region is down and you need to perform maintenance on the other. Three regions gives operational flexibility and satisfies N+1 during maintenance windows.' },
        ],
      },
      {
        id: 'task-2',
        title: 'Zero-RPO database architecture',
        description: 'Banking transactions cannot lose a single byte. How do you achieve zero RPO for the database layer?',
        options: [
          { id: 'a', text: 'Cloud Spanner with multi-region configuration for synchronous replication across regions', points: 10, feedback: 'Cloud Spanner is the only GCP database that provides synchronous multi-region replication with strong consistency. It guarantees zero data loss during regional failures. The trade-off is slightly higher write latency due to cross-region consensus, which is acceptable for banking correctness guarantees.' },
          { id: 'b', text: 'Cloud SQL with cross-region read replicas and async replication', points: 4, feedback: 'Async replication means data written to the primary but not yet replicated is lost during failover. For a bank, even seconds of data loss (transactions, transfers) is unacceptable. This doesn\'t achieve zero RPO.' },
          { id: 'c', text: 'Cloud SQL with custom synchronous replication scripts', points: 2, feedback: 'Custom replication is fragile, hard to maintain, and hasn\'t been tested at scale. For banking-critical data, use a database that provides synchronous replication natively (Cloud Spanner).' },
          { id: 'd', text: 'AlloyDB with cross-region replication', points: 6, feedback: 'AlloyDB offers good performance and some replication features, but doesn\'t provide the same synchronous multi-region consistency guarantees as Cloud Spanner. For zero RPO banking requirements, Spanner is the more reliable choice.' },
        ],
      },
      {
        id: 'task-3',
        title: 'Automated failover design',
        description: 'When a region fails, failover must complete within 15 minutes. How do you automate this?',
        options: [
          { id: 'a', text: 'Health check-driven automatic failover via Global LB, with Cloud Spanner handling data layer automatically, and Cloud Functions for custom failover logic (DNS, cache warming)', points: 10, feedback: 'Fully automated approach! The Global LB detects unhealthy backends and reroutes traffic automatically. Spanner handles database failover transparently. Cloud Functions can trigger custom steps like DNS updates, notification sends, and cache pre-warming. Total failover time: seconds for traffic, minutes for full stabilization.' },
          { id: 'b', text: 'Manual runbook executed by on-call engineer', points: 3, feedback: 'Manual failover adds human time to the process: wake up, assess, execute steps. This easily exceeds 15 minutes, especially at 3 AM. And human error during high-stress incidents is common.' },
          { id: 'c', text: 'Automated failover for compute, manual for database', points: 5, feedback: 'Splitting automation creates inconsistency. If compute fails over but the database doesn\'t, the application connects to the wrong database or fails entirely. Automate the full stack.' },
          { id: 'd', text: 'Terraform scripts to rebuild infrastructure in a new region', points: 2, feedback: 'Rebuilding infrastructure during a disaster takes far longer than 15 minutes. Pre-provisioned active-active infrastructure eliminates rebuild time entirely.' },
        ],
      },
      {
        id: 'task-4',
        title: 'Data backup and recovery',
        description: 'Beyond real-time replication, regulatory compliance requires long-term backups. How do you implement backup strategy?',
        options: [
          { id: 'a', text: 'Spanner automated backups + Cloud Storage dual-region for long-term archives + point-in-time recovery enabled + cross-project backup copies', points: 10, feedback: 'Defense in depth! Spanner backups protect against logical errors (bad code, accidental deletes). Dual-region Cloud Storage for archives satisfies long-term retention requirements. Point-in-time recovery enables restoration to any moment. Cross-project copies protect against project-level compromise.' },
          { id: 'b', text: 'Spanner automated backups only', points: 5, feedback: 'Spanner backups protect against logical errors but if the Spanner instance or project is compromised, backups may be affected too. You need off-platform copies and long-term archival for regulatory compliance.' },
          { id: 'c', text: 'Nightly exports to Cloud Storage', points: 4, feedback: 'Nightly exports mean up to 24 hours of data loss for recovery scenarios. For a bank, you need continuous backups with point-in-time recovery. Nightly exports can supplement but not replace real-time backups.' },
          { id: 'd', text: 'Real-time replication is our backup', points: 2, feedback: 'Replication protects against hardware failures but not logical errors (corrupted data, accidental deletes, malicious changes). If bad data is written, it\'s replicated everywhere instantly. You need point-in-time backups to recover from logical errors.' },
        ],
      },
      {
        id: 'task-5',
        title: 'DR testing strategy',
        description: 'Regulators require proof that DR works. How do you validate the DR architecture?',
        options: [
          { id: 'a', text: 'Quarterly full DR drills (simulate region failure), monthly partial tests, automated chaos engineering in staging, and documented results for auditors', points: 10, feedback: 'Regulatory best practice! Quarterly full drills prove end-to-end recovery. Monthly partial tests validate individual components. Continuous chaos engineering catches regression. Documented results create audit trails. Some banks do these during business hours to prove real-world readiness.' },
          { id: 'b', text: 'Annual DR test during a maintenance window', points: 4, feedback: 'Annual testing is too infrequent. Configuration drift, new services, and code changes can break DR between tests. By the time you discover issues, it may be too late.' },
          { id: 'c', text: 'Test in staging environment only', points: 3, feedback: 'Staging doesn\'t reflect production scale, configuration, or data volume. DR procedures that work in staging may fail in production. Test where it matters.' },
          { id: 'd', text: 'Automated failover tests in production monthly', points: 7, feedback: 'Monthly production tests are good but you also need full DR drills that simulate extended outages. Automated tests validate the mechanism; full drills validate the people and processes too.' },
        ],
      },
      {
        id: 'task-6',
        title: 'Compliance and audit readiness',
        description: 'Bank regulators need to audit your DR capability. How do you ensure audit readiness?',
        options: [
          { id: 'a', text: 'Automated compliance monitoring (Security Command Center), immutable audit logs (Cloud Logging with locked retention), DR test reports, architecture diagrams in version control, and regular third-party assessments', points: 10, feedback: 'Complete audit readiness! Automated compliance monitoring catches drift. Immutable logs prove actions were taken. DR test reports demonstrate capability. Version-controlled architecture docs show change history. Third-party assessments provide independent validation.' },
          { id: 'b', text: 'Documentation of the DR architecture', points: 4, feedback: 'Documentation is necessary but not sufficient. Auditors want evidence of testing, logs of actual failovers, and proof that monitoring catches issues. Static documentation goes stale quickly.' },
          { id: 'c', text: 'Cloud Audit Logs with a summary report', points: 5, feedback: 'Audit logs are one piece of the puzzle. Regulators want to see the full picture: architecture decisions, test results, monitoring effectiveness, and incident response capability.' },
          { id: 'd', text: 'Hire a compliance team to handle audits', points: 3, feedback: 'A compliance team is helpful but they need the technical evidence to present. Without automated monitoring, test reports, and immutable logs, the compliance team has nothing to show auditors.' },
        ],
      },
    ],
  },
]
